/**
 * Generated by "@vuepress/internal-site-data"
 */
export const siteData = {
  "title": "ljh blog",
  "description": "welcomg to my blog",
  "base": "/myblog/",
  "headTags": [
    [
      "link",
      {
        "rel": "stylesheet",
        "href": "https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"
      }
    ],
    [
      "link",
      {
        "rel": "stylesheet",
        "href": "https://cdn.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css"
      }
    ]
  ],
  "pages": [
    {
      "title": "Home",
      "frontmatter": {
        "home": true,
        "heroImage": "/hero.png",
        "actionText": "快速上手 →",
        "actionLink": "/all/all",
        "features": [
          {
            "title": "读书",
            "details": "书籍的力量"
          },
          {
            "title": "技术",
            "details": "码农的记录"
          },
          {
            "title": "生活",
            "details": "随笔随想"
          }
        ],
        "footer": "MIT Licensed | Copyright © 2024-present ljh"
      },
      "regularPath": "/",
      "relativePath": "README.md",
      "key": "v-90123fb0",
      "path": "/",
      "headersStr": null,
      "content": "",
      "normalizedContent": "",
      "charsets": {}
    },
    {
      "frontmatter": {},
      "regularPath": "/exp/idea.html",
      "relativePath": "exp/idea.md",
      "key": "v-1f39de00",
      "path": "/exp/idea.html",
      "headersStr": null,
      "content": " * 实验1\n   \n   根据单模态一致性的emotion进行过滤得到的valence的distance均值会更加小，这种一致性是基于是否与文本模态一致进行判别的。\n   \n   >  * 需要考虑的是生成的标签是否存在长尾\n   > \n   >  * 准确率指标是难以再有提升的了\n\n * 想法2\n   \n   直接用文本模态的预测引导音频和视觉\n   \n   > 每个样本级别的损失，100个\n   > \n   > 统计结果，有67%的样本，文本好于语音；有75%的样本，文本好于视觉\n\n * 想法3\n   \n   之前模态不平衡的问题是不是因为过拟合方面的原因？\n   \n   > mosei数据集，过拟合文本模态\n   > \n   > mosi数据集，过拟合语音模态\n\n * 想法4 $$ Score_{m} = full_{acc} - remove(m)_{acc} $$\n\n * 想法5\n   \n   数据增强，对增强后的数据(称为restricted view)进行与single modality feature的累加进行distill\n\n * distill\n   \n   模型有没有学到交互信息？怎么定义是否学到交互信息？有的话需要保留特质信息吗\n\n * 想法6\n   \n   过拟合就不要再训练了\n\n * inconsistency\n   \n   self-mm,label,提取出不一致的/人工筛选，然后做实验，英语数据集上不一致的似乎还没有\n   \n   不一致问题本质上可不可以通过贡献来衡量\n\n * modality noise/missing\n   \n   * video:blank screen,gaussian-blur\n   * audio:mute,noise white/metro/office/park/diner\n   * text:replace,remove",
      "normalizedContent": " * 实验1\n   \n   根据单模态一致性的emotion进行过滤得到的valence的distance均值会更加小，这种一致性是基于是否与文本模态一致进行判别的。\n   \n   >  * 需要考虑的是生成的标签是否存在长尾\n   > \n   >  * 准确率指标是难以再有提升的了\n\n * 想法2\n   \n   直接用文本模态的预测引导音频和视觉\n   \n   > 每个样本级别的损失，100个\n   > \n   > 统计结果，有67%的样本，文本好于语音；有75%的样本，文本好于视觉\n\n * 想法3\n   \n   之前模态不平衡的问题是不是因为过拟合方面的原因？\n   \n   > mosei数据集，过拟合文本模态\n   > \n   > mosi数据集，过拟合语音模态\n\n * 想法4 $$ score_{m} = full_{acc} - remove(m)_{acc} $$\n\n * 想法5\n   \n   数据增强，对增强后的数据(称为restricted view)进行与single modality feature的累加进行distill\n\n * distill\n   \n   模型有没有学到交互信息？怎么定义是否学到交互信息？有的话需要保留特质信息吗\n\n * 想法6\n   \n   过拟合就不要再训练了\n\n * inconsistency\n   \n   self-mm,label,提取出不一致的/人工筛选，然后做实验，英语数据集上不一致的似乎还没有\n   \n   不一致问题本质上可不可以通过贡献来衡量\n\n * modality noise/missing\n   \n   * video:blank screen,gaussian-blur\n   * audio:mute,noise white/metro/office/park/diner\n   * text:replace,remove",
      "charsets": {
        "cjk": true
      }
    },
    {
      "frontmatter": {},
      "regularPath": "/about/",
      "relativePath": "about/README.md",
      "key": "v-ac3de64e",
      "path": "/about/",
      "headersStr": null,
      "content": "热爱生活",
      "normalizedContent": "热爱生活",
      "charsets": {
        "cjk": true
      }
    },
    {
      "title": "训练指令",
      "frontmatter": {},
      "regularPath": "/exp/training_order.html",
      "relativePath": "exp/training_order.md",
      "key": "v-14d478dc",
      "path": "/exp/training_order.html",
      "headers": [
        {
          "level": 3,
          "title": "训练指令",
          "slug": "训练指令",
          "normalizedTitle": "训练指令",
          "charIndex": 2
        },
        {
          "level": 3,
          "title": "可视化指令",
          "slug": "可视化指令",
          "normalizedTitle": "可视化指令",
          "charIndex": 464
        },
        {
          "level": 3,
          "title": "对齐操作",
          "slug": "对齐操作",
          "normalizedTitle": "对齐操作",
          "charIndex": 811
        },
        {
          "level": 3,
          "title": "自训练",
          "slug": "自训练",
          "normalizedTitle": "自训练",
          "charIndex": 1081
        },
        {
          "level": 3,
          "title": "画图",
          "slug": "画图",
          "normalizedTitle": "画图",
          "charIndex": 2530
        }
      ],
      "headersStr": "训练指令 可视化指令 对齐操作 自训练 画图",
      "content": "# 训练指令\n\ntrain.sh 文件下修改运行train.py的参数，如--nothing,--dataset,--fusion_mode\n\ntee：保存命令行的输出结果，结果保存在./trainlog_mosei/mylog.log路径下，-a代表以追加的形式\n\nbash train_origin.sh --nothing --dataset mosei --fusion_mode inter_intra 2>&1 | tee -a ./trainlog_mosei/mylog.log\n\n\nbash train_tcp.sh 2>&1 | tee -a ./trainlog_mosei/mylog_tcp.log\n\n\n消融实验\n\nbash train_origin.sh --nothing --dataset mosei --fusion_mode inter_intra --without_modality t 2>&1 | tee -a ./trainlog_mosei/mylog_wt.log\n\n\n\n# 可视化指令\n\nvisualize.sh文件下，可视化注意力机制的结果，通过运行visualize.py文件，可指定参数 --visualize 样本名，得到对应样本的attention_weight，为一字典类型，包含global（fusion_mode为global时）; inter，intra（fusion_mode为inter_intra时）模型的注意力结果，shape为(layers, bsz, num_heads, q_len, kv_len)\n\n总共涉及到2个文件：visualize.py，model_attenvisualize.py\n\nbash visualize.sh 2>&1 | tee -a ./visualizelog_mosei/mylog.log\n\n\n\n# 对齐操作\n\n 1. 首先，使用d\\数据集\\mosei\\raw\\audio\\full目录下的whisper_rec.py获取所需要得到time_stamps的音频文件（修改音频路径，存储路径）\n\n 2. 接着运行服务器上目录为**/autodl-tmp/alpaca-lora/ljh/code/**下的sim2.py文件，获取得到对齐结果，存储在pkl文件中（修改存储路径）\n\n 3. 接着本地对视频（d\\数据集\\mosei\\load.py）、音频进行clip（d\\数据集\\mosei\\clip-audio.py）（修改存储路径）\n\n\n# 自训练\n\n基于预训练模型，跑一遍推理；输出csv文件，每类选取top_k个样本（confidence），扩充训练集，重新训练模型，冻结regression头，重新训练p次\n\n首先进行预训练，通过修改模型参数保存路径，保存到指定路径下\n\nbash train.sh 2>&1 | tee -a ./trainlog_mosei/mylog.log\n\n\n从指定路径下加载模型参数，运行以下指令，会读取/root/autodl-tmp/alpaca-lora/ljh/DPC_KNN/data/ami/ami_audiofilter.csv文件，推理ami数据集，得到伪标签，保存结果在/root/autodl-tmp/alpaca-lora/ljh/code/distill_gate/SFIN_origin/ami_result/ami_pred.csv文件中，运行目录下的analysis.py文件，会得到滤除后的top-10样本的预测，即每个类别选取10个高置信度的样本，最终得到filter_ami_pred.csv文件（以追加的形式），基于该文件重新训练模型，指令的运行顺序如下：\n\nbash train.sh 2>&1 | tee -a ./trainlog_mosei/mylog.log # 模型参数保存路径指定，每次重新训练路径最好保持不一致\npython inf_ami_args.py # 输入ami_audiofilter.csv，模型参数；输出：ami_pred.csv\npython analysis.py # 输入：ami_pred.csv 追加的形式输出：filter_ami_pred.csv，同时更新ami_auidofilter.csv,即剩余的未推理的ami样本\nbash train.sh 2>&1 | tee -a ./trainlog_mosei/mylog.log # 输入：filter_ami_pred.csv，模型参数保存路径指定，指定为retrain模式\npython inf_ami_args.py # 输入ami_audiofilter.csv，模型参数；输出：ami_pred.csv\npython analysis.py # 输入：ami_pred.csv 追加的形式输出：filter_ami_pred.csv，同时更新ami_auidofilter.csv,即剩余的未推理的ami样本\n\n\n现在retrain的想法是，val head损失由mosei数据集提供，emo head的损失由mosei+ami数据集提供。\n\n已封装成相关指令，循环自训练的指令\n\nbash train.sh --nothing --dataset mosei --fusion_mode inter_intra --retrain 2>&1 | tee -a ./trainlog_mosei/mylog.log\n\n\n每次重新自训练前，需要更新初始模型，路径在model_single/mosei/inter_intramodelf1.pt中，需更换为初始模型，路径在model_single_originvideo/mosei/inter_intramodelf.pt，更换**ami_audiofilter.csv为初始版本，删除ami_result/ami_pred.csv，ami_result/filter_ami_pred.csv**\n\n\n# 画图\n\n让坐标轴的label始终对准每一小格的中间，思路：首先得到每一个grid的size，然后获取坐标的起止，让每个刻度都加grid_size/2，终止刻度也要加0.5，传入即可\n\n# 获取横坐标轴的范围\nx_min, x_max = plt.xlim()# -0.5 0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 10.5 11.5 12.5 13.5\n# 计算一个小格的长度\ngrid_size = (x_max - x_min) / len(words) #word为每一小格放置的label\nticks = np.arange(x_min + grid_size/2, x_max + grid_size/2 , grid_size)\n\n\n",
      "normalizedContent": "# 训练指令\n\ntrain.sh 文件下修改运行train.py的参数，如--nothing,--dataset,--fusion_mode\n\ntee：保存命令行的输出结果，结果保存在./trainlog_mosei/mylog.log路径下，-a代表以追加的形式\n\nbash train_origin.sh --nothing --dataset mosei --fusion_mode inter_intra 2>&1 | tee -a ./trainlog_mosei/mylog.log\n\n\nbash train_tcp.sh 2>&1 | tee -a ./trainlog_mosei/mylog_tcp.log\n\n\n消融实验\n\nbash train_origin.sh --nothing --dataset mosei --fusion_mode inter_intra --without_modality t 2>&1 | tee -a ./trainlog_mosei/mylog_wt.log\n\n\n\n# 可视化指令\n\nvisualize.sh文件下，可视化注意力机制的结果，通过运行visualize.py文件，可指定参数 --visualize 样本名，得到对应样本的attention_weight，为一字典类型，包含global（fusion_mode为global时）; inter，intra（fusion_mode为inter_intra时）模型的注意力结果，shape为(layers, bsz, num_heads, q_len, kv_len)\n\n总共涉及到2个文件：visualize.py，model_attenvisualize.py\n\nbash visualize.sh 2>&1 | tee -a ./visualizelog_mosei/mylog.log\n\n\n\n# 对齐操作\n\n 1. 首先，使用d\\数据集\\mosei\\raw\\audio\\full目录下的whisper_rec.py获取所需要得到time_stamps的音频文件（修改音频路径，存储路径）\n\n 2. 接着运行服务器上目录为**/autodl-tmp/alpaca-lora/ljh/code/**下的sim2.py文件，获取得到对齐结果，存储在pkl文件中（修改存储路径）\n\n 3. 接着本地对视频（d\\数据集\\mosei\\load.py）、音频进行clip（d\\数据集\\mosei\\clip-audio.py）（修改存储路径）\n\n\n# 自训练\n\n基于预训练模型，跑一遍推理；输出csv文件，每类选取top_k个样本（confidence），扩充训练集，重新训练模型，冻结regression头，重新训练p次\n\n首先进行预训练，通过修改模型参数保存路径，保存到指定路径下\n\nbash train.sh 2>&1 | tee -a ./trainlog_mosei/mylog.log\n\n\n从指定路径下加载模型参数，运行以下指令，会读取/root/autodl-tmp/alpaca-lora/ljh/dpc_knn/data/ami/ami_audiofilter.csv文件，推理ami数据集，得到伪标签，保存结果在/root/autodl-tmp/alpaca-lora/ljh/code/distill_gate/sfin_origin/ami_result/ami_pred.csv文件中，运行目录下的analysis.py文件，会得到滤除后的top-10样本的预测，即每个类别选取10个高置信度的样本，最终得到filter_ami_pred.csv文件（以追加的形式），基于该文件重新训练模型，指令的运行顺序如下：\n\nbash train.sh 2>&1 | tee -a ./trainlog_mosei/mylog.log # 模型参数保存路径指定，每次重新训练路径最好保持不一致\npython inf_ami_args.py # 输入ami_audiofilter.csv，模型参数；输出：ami_pred.csv\npython analysis.py # 输入：ami_pred.csv 追加的形式输出：filter_ami_pred.csv，同时更新ami_auidofilter.csv,即剩余的未推理的ami样本\nbash train.sh 2>&1 | tee -a ./trainlog_mosei/mylog.log # 输入：filter_ami_pred.csv，模型参数保存路径指定，指定为retrain模式\npython inf_ami_args.py # 输入ami_audiofilter.csv，模型参数；输出：ami_pred.csv\npython analysis.py # 输入：ami_pred.csv 追加的形式输出：filter_ami_pred.csv，同时更新ami_auidofilter.csv,即剩余的未推理的ami样本\n\n\n现在retrain的想法是，val head损失由mosei数据集提供，emo head的损失由mosei+ami数据集提供。\n\n已封装成相关指令，循环自训练的指令\n\nbash train.sh --nothing --dataset mosei --fusion_mode inter_intra --retrain 2>&1 | tee -a ./trainlog_mosei/mylog.log\n\n\n每次重新自训练前，需要更新初始模型，路径在model_single/mosei/inter_intramodelf1.pt中，需更换为初始模型，路径在model_single_originvideo/mosei/inter_intramodelf.pt，更换**ami_audiofilter.csv为初始版本，删除ami_result/ami_pred.csv，ami_result/filter_ami_pred.csv**\n\n\n# 画图\n\n让坐标轴的label始终对准每一小格的中间，思路：首先得到每一个grid的size，然后获取坐标的起止，让每个刻度都加grid_size/2，终止刻度也要加0.5，传入即可\n\n# 获取横坐标轴的范围\nx_min, x_max = plt.xlim()# -0.5 0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 10.5 11.5 12.5 13.5\n# 计算一个小格的长度\ngrid_size = (x_max - x_min) / len(words) #word为每一小格放置的label\nticks = np.arange(x_min + grid_size/2, x_max + grid_size/2 , grid_size)\n\n\n",
      "charsets": {
        "cjk": true
      }
    },
    {
      "title": "实验",
      "frontmatter": {},
      "regularPath": "/exp/",
      "relativePath": "exp/README.md",
      "key": "v-8e3e550e",
      "path": "/exp/",
      "headersStr": null,
      "content": "**想法：**记录一些从阅读论文中汲取得到的想法，灵感等。\n\n**训练指令：**记录一些实验上的训练命令。",
      "normalizedContent": "**想法：**记录一些从阅读论文中汲取得到的想法，灵感等。\n\n**训练指令：**记录一些实验上的训练命令。",
      "charsets": {
        "cjk": true
      }
    },
    {
      "title": "A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations",
      "frontmatter": {},
      "regularPath": "/paper/paper.html",
      "relativePath": "paper/paper.md",
      "key": "v-9b792bf4",
      "path": "/paper/paper.html",
      "headers": [
        {
          "level": 3,
          "title": "A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations",
          "slug": "a-facial-expression-aware-multimodal-multi-task-learning-framework-for-emotion-recognition-in-multi-party-conversations",
          "normalizedTitle": "a facial expression-aware multimodal multi-task learning framework for emotion recognition in multi-party conversations",
          "charIndex": 2
        },
        {
          "level": 3,
          "title": "EMT_DLFR",
          "slug": "emt-dlfr",
          "normalizedTitle": "emt_dlfr",
          "charIndex": 368
        },
        {
          "level": 3,
          "title": "Robust-MSA",
          "slug": "robust-msa",
          "normalizedTitle": "robust-msa",
          "charIndex": 588
        },
        {
          "level": 3,
          "title": "NIAT",
          "slug": "niat",
          "normalizedTitle": "niat",
          "charIndex": 716
        },
        {
          "level": 3,
          "title": "UniMF",
          "slug": "unimf",
          "normalizedTitle": "unimf",
          "charIndex": 843
        },
        {
          "level": 3,
          "title": "半监督的paper",
          "slug": "半监督的paper",
          "normalizedTitle": "半监督的paper",
          "charIndex": 1080
        },
        {
          "level": 3,
          "title": "my paper",
          "slug": "my-paper",
          "normalizedTitle": "my paper",
          "charIndex": 1207
        },
        {
          "level": 3,
          "title": "meta_noise",
          "slug": "meta-noise",
          "normalizedTitle": "meta_noise",
          "charIndex": 1561
        },
        {
          "level": 3,
          "title": "mask autoencoder",
          "slug": "mask-autoencoder",
          "normalizedTitle": "mask autoencoder",
          "charIndex": 1663
        }
      ],
      "headersStr": "A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations EMT_DLFR Robust-MSA NIAT UniMF 半监督的paper my paper meta_noise mask autoencoder",
      "content": "# A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations\n\n----------------------------------------\n\n一段视频输入talknet得到可能的说话人的帧序列（有可能是错误的，因此需要后续的匹配操作），将这些帧序列进行聚类，由于meld数据集包含6个不同的说话人，因此，预先从数据集中得到每个人20张不同角度的人脸图片，针对聚类完成得到可能说话人的帧序列，经由resnet-50(pretrained on face recongnition dataset)提取对应的特征，计算相似度得到最终的结果\n\n\n\n\n# EMT_DLFR\n\n----------------------------------------\n\n前向两次，分别得到完整模态，缺失模态的结果\n\n在不同epoch下的同一样本，缺失的time step是一样的。而cutoff则是不一样的。能否在训练时人为构造更多的缺失样本，一方面可以避免过拟合，一方面模拟更多真实世界的缺失模态情况\n\n跨层参数共享，global2local的attention,将注意力的计算降低至线性复杂度\n\n\n# Robust-MSA\n\n----------------------------------------\n\n端到端的方式；情感词的识别与replace；asr error；语音的背景噪声（人声分离？）；图像中人脸角度，不同光照，人脸缺失，运动模糊\n\n\n# NIAT\n\n----------------------------------------\n\n三种模态在对齐的情况下，设置了三种不一样的缺失情况：三种模态缺失的位置均不一致（随机位置缺失）；三种模态缺失的位置一致（连续片段缺失，随机位置缺失）\n\n\n# UniMF\n\n----------------------------------------\n\n提出不同的mask机制，解决未对齐序列缺失模态的问题\n\n图中橘色部分有： $$ Y_{[multi]} = CA_{L->[multi]}([multi],Z_{L})+CA_{A->[multi]}([multi],Z_{A})+SA([multi]) $$\n\n> 构造两种不同的mask,一种学习模态内部的交互，一种学习模态间的交互。并采用balence策略\n\n\n\n\n# 半监督的paper\n\n----------------------------------------\n\n第一篇，关键是balence所生成的样本类别数目，作者对每一类选取top-k置信度的样本，保证了重新训练时样本数量的平衡\n\n * 门控机制\n\n\n# my paper\n\n----------------------------------------\n\n单一视角（intra） vs 多视角（inter）\n\n有的时候，我们仅仅通过单一一种模态，便能做出很好的感情推断，无需其他模态的帮助：\n\n> only inf from text:\n> \n>  1. a lot of sad parts\n>  2. there is sad part\n\n但有时我们需要借助其他模态的帮助(交互信息)从而准确判断主体的情感：\n\n> inf from text and visual:\n> \n> netural text: AND THERE BUT THEYRE NOT HUGE GLARING FLAWS\n> \n> negative video: frown\n\n\n# meta_noise\n\n> 元学习相关资料：https://zhuanlan.zhihu.com/p/72920138，https://zhuanlan.zhihu.com/p/57864886\n\n\n# mask autoencoder\n\n> https://zhuanlan.zhihu.com/p/439554945\n\n * 离线数据增强与在线数据增强\n\n * xlnet\n\n * top-k准确率的计算\n   \n   > https://zhuanlan.zhihu.com/p/340760336\n\n * 项目使用的分类代码\n   \n   > https://github.com/bubbliiiing/classification-pytorch",
      "normalizedContent": "# a facial expression-aware multimodal multi-task learning framework for emotion recognition in multi-party conversations\n\n----------------------------------------\n\n一段视频输入talknet得到可能的说话人的帧序列（有可能是错误的，因此需要后续的匹配操作），将这些帧序列进行聚类，由于meld数据集包含6个不同的说话人，因此，预先从数据集中得到每个人20张不同角度的人脸图片，针对聚类完成得到可能说话人的帧序列，经由resnet-50(pretrained on face recongnition dataset)提取对应的特征，计算相似度得到最终的结果\n\n\n\n\n# emt_dlfr\n\n----------------------------------------\n\n前向两次，分别得到完整模态，缺失模态的结果\n\n在不同epoch下的同一样本，缺失的time step是一样的。而cutoff则是不一样的。能否在训练时人为构造更多的缺失样本，一方面可以避免过拟合，一方面模拟更多真实世界的缺失模态情况\n\n跨层参数共享，global2local的attention,将注意力的计算降低至线性复杂度\n\n\n# robust-msa\n\n----------------------------------------\n\n端到端的方式；情感词的识别与replace；asr error；语音的背景噪声（人声分离？）；图像中人脸角度，不同光照，人脸缺失，运动模糊\n\n\n# niat\n\n----------------------------------------\n\n三种模态在对齐的情况下，设置了三种不一样的缺失情况：三种模态缺失的位置均不一致（随机位置缺失）；三种模态缺失的位置一致（连续片段缺失，随机位置缺失）\n\n\n# unimf\n\n----------------------------------------\n\n提出不同的mask机制，解决未对齐序列缺失模态的问题\n\n图中橘色部分有： $$ y_{[multi]} = ca_{l->[multi]}([multi],z_{l})+ca_{a->[multi]}([multi],z_{a})+sa([multi]) $$\n\n> 构造两种不同的mask,一种学习模态内部的交互，一种学习模态间的交互。并采用balence策略\n\n\n\n\n# 半监督的paper\n\n----------------------------------------\n\n第一篇，关键是balence所生成的样本类别数目，作者对每一类选取top-k置信度的样本，保证了重新训练时样本数量的平衡\n\n * 门控机制\n\n\n# my paper\n\n----------------------------------------\n\n单一视角（intra） vs 多视角（inter）\n\n有的时候，我们仅仅通过单一一种模态，便能做出很好的感情推断，无需其他模态的帮助：\n\n> only inf from text:\n> \n>  1. a lot of sad parts\n>  2. there is sad part\n\n但有时我们需要借助其他模态的帮助(交互信息)从而准确判断主体的情感：\n\n> inf from text and visual:\n> \n> netural text: and there but theyre not huge glaring flaws\n> \n> negative video: frown\n\n\n# meta_noise\n\n> 元学习相关资料：https://zhuanlan.zhihu.com/p/72920138，https://zhuanlan.zhihu.com/p/57864886\n\n\n# mask autoencoder\n\n> https://zhuanlan.zhihu.com/p/439554945\n\n * 离线数据增强与在线数据增强\n\n * xlnet\n\n * top-k准确率的计算\n   \n   > https://zhuanlan.zhihu.com/p/340760336\n\n * 项目使用的分类代码\n   \n   > https://github.com/bubbliiiing/classification-pytorch",
      "charsets": {
        "cjk": true
      }
    },
    {
      "title": "论文",
      "frontmatter": {},
      "regularPath": "/paper/",
      "relativePath": "paper/README.md",
      "key": "v-1c80ce9b",
      "path": "/paper/",
      "headersStr": null,
      "content": "",
      "normalizedContent": "",
      "charsets": {}
    },
    {
      "frontmatter": {},
      "regularPath": "/all/all.html",
      "relativePath": "all/all.md",
      "key": "v-cd71c7f4",
      "path": "/all/all.html",
      "headersStr": null,
      "content": "",
      "normalizedContent": "",
      "charsets": {}
    },
    {
      "frontmatter": {},
      "regularPath": "/paper/paper_writing.html",
      "relativePath": "paper/paper_writing.md",
      "key": "v-8bd484b4",
      "path": "/paper/paper_writing.html",
      "headersStr": null,
      "content": "关于splncs04引用格式，该风格是根据文章作者首字母进行排序后进行引用，有时候我们不需要排序，而是根据引用顺序在文章中标号即可，因此需要修改splncs04文件，将SORTZ注释改为splncs04_unsort格式即可\n\n> https://zhuanlan.zhihu.com/p/427371287",
      "normalizedContent": "关于splncs04引用格式，该风格是根据文章作者首字母进行排序后进行引用，有时候我们不需要排序，而是根据引用顺序在文章中标号即可，因此需要修改splncs04文件，将sortz注释改为splncs04_unsort格式即可\n\n> https://zhuanlan.zhihu.com/p/427371287",
      "charsets": {
        "cjk": true
      }
    },
    {
      "title": "*",
      "frontmatter": {},
      "regularPath": "/prepare4work/pre4work.html",
      "relativePath": "prepare4work/pre4work.md",
      "key": "v-2c8f2900",
      "path": "/prepare4work/pre4work.html",
      "headers": [
        {
          "level": 3,
          "title": "",
          "slug": "",
          "normalizedTitle": "",
          "charIndex": 0
        },
        {
          "level": 3,
          "title": "idea",
          "slug": "idea",
          "normalizedTitle": "idea",
          "charIndex": 605
        },
        {
          "level": 3,
          "title": "code",
          "slug": "code",
          "normalizedTitle": "code",
          "charIndex": 1730
        }
      ],
      "headersStr": " idea code",
      "content": "#\n\n----------------------------------------\n\n 1. 跨层参数共享\n\n> 优点：需保存的参数量降低，减少存储成本。减少训练时间\n> \n> 缺点：推理时间仍然一样\n\n> 共享不同参数层的结果\n\n 2. 词向量因式分解\n    \n    > 引入一全连接层，将词向量维度降维，达到降低transformer中隐层维度的目的\n\n 3. 渐进式知识蒸馏\n    \n    > MobileBERT:学生模型开始学习教师模型的第一层，接下来学习教师模型的第二层，而此时学生模型的第一层权重是不参与更新的。依此类推。\n\n 4. 注意力机制的改进（longformer）\n    \n    >  * 滑动窗口注意力\n    >    \n    >    当前token仅与左右k/2个token窗口内的token计算注意力\n    > \n    >  * 扩张窗口注意力\n    >    \n    >    在窗口内不是与所有token计算注意力而是进行采样，与窗口内被采样到的token计算注意力（采样间隔）\n    > \n    >  * 全局+滑动窗口\n    >    \n    >    将QKV映射两种不同的子空间，这两种不同的QK,分别计算全局（与具体任务有关，如具有全局视角的CLS，则该token可以关注其他所有token的信息）和滑动窗口注意力\n\n\n\n\n# idea\n\n----------------------------------------\n\n 1. 缺失模态下的情感分析\n\n> 首先，预训练，基于MLM策略（动态掩码（Robeta）/静态掩码）\n> \n> 接着，基于预训练好的的重建模型，提取特征，模态融合模块，进行情感预测\n\n 2. 相关的掩码策略（NLP，BART）\n\n * 单词掩码，在输入文本中随机采样一部分单词，并且替换为掩码标记如[MASK]\n\n * 单词删除，随机采样一部分单词并且删除。在预测缺失单词的同时还需要确定缺失单词的位置\n\n * 句子排列变换。将多个句子的顺序随机打乱\n\n * 文档旋转变换\n\n * 文本填充\n\n * 实体级别掩码，在普通的单词掩码中，模型只需要根据J.与Rowling就可以做出正确的预测，而无法学习到更高层次的语义\n   \n   \n   \n   \n\n 3. 门控策略\n\n * 输入信号经非线性变换映射至[0,1]，得到门控向量，与原始输入进行元素点乘\n\n 4. bert相关\n\n * 词向量编码，块编码，位置编码\n   \n   > 词向量编码由独热向量经由词向量矩阵（可学习参数矩阵）映射得到\n   > \n   > 块编码指的是当前token属于哪一句子，属于句子0则编码为0，属于句子1则编码为1，以此类推。运用块向量矩阵进行映射得到最终的块编码\n   > \n   > 位置编码则是将按单词顺序转换为独热编码，再经由位置向量矩阵转换为位置编码\n\n * MLM实现\n   \n   在得到掩码位置的隐向量后，经由词向量矩阵，映射到词表空间，之后经softmax操作得到词表中的词对应该掩码位置的概率，取max即为预测的单词，模型对第i个掩码位置的预测概率的计算如下： $$ P(i)=Softmax(h_{i}W_{}^{T}+b) $$\n   \n   > 整词掩码：掩码的最小粒度为单词级而非wordpiece级，提升了mlm任务的难度，模型在预测掩码词时，需要更多依靠上下文信息，有助于提升模型对输入文本语义信息的挖掘\n   > \n   > 需要注意的是，整词掩码中，子词的掩码方式可以是替换为[MASK]，随机词或者保留原词，三选一；各个子词均会受到掩码；掩码的方式受到概率控制\n\n * 下游任务\n   \n   抽取式阅读理解：将问题和篇章输入至模型，问题在前，篇章在后，篇章超过最大允许输入长度时，分段输入。在获取得到每个位置起点、终点位置的概率，分别取top-k个，假设起点终点位置相互独立，则联合概率为二者相乘，得到一个[start,end,prob]的三元组，数量为k*k个，添加限制条件start<end，筛选得到最终的结果。\n\n\n# code\n\n----------------------------------------\n\n 1. 链表\n    \n    > 环的入口节点，如果一链表中包含环，如何确定环的起点\n    > \n    > \n    > \n    > 解法：双指针，通过快慢指针找到处于环中的节点，快指针每次走两步，慢指针每次走一步，由于链表存在环，因此最终肯定会碰上（快的会追上慢的，rectify:快指针多走n步，随后两个指针每次都是走一步），重复的位置就是处于环中的节点。接着如果快慢指针每次走的步长相差为环的长度的整数倍，则最终必会在入口处碰上(rectify:初始时，快指针多走环的长度的步长，接着每次都走一步)，问题转化为寻找环的长度的问题，环的长度可以通过单指针实现，在步骤一中得到的起始节点出发，走一遍，并进行计数，最终就能得到环的长度。接着进行步骤二就能找到入口节点。\n\n----------------------------------------\n\n> 链表中倒数第K个节点\n> \n> 解法：双指针，快指针初始多走k（k-1）步，接着每次都走一步，快指针到终点时，慢指针刚好指向倒数第k个节点。\n\n----------------------------------------\n\n> 调整数组顺序使奇数位于偶数前面\n> \n> 解法：双指针，奇偶（分别向中心移动），奇奇（while到奇数左指针+1），偶偶（右指针-1），偶奇（交换）\n> \n> 2，1，4，5，3\n> \n> 3，1，4，5，2\n> \n> 3，1，5，4，2\n\n----------------------------------------\n\n>  * 删除链表的节点：给定单向链表的头指针和一个节点指针，定义一个函数在O(1)时间内删除该节点\n> \n> 解法：有了要删除的节点指针，可以知道该节点的下一个节点，把下一个节点的值赋值给要删除的节点，将当前节点的next指向下下个节点，除此之外还需要把待删除节点指向的下一个节点删除。倘若在尾部，只能顺序查找删除\n> \n> \n> \n> 时间复杂度的分析\n> \n> ((n-1)*O(1)+O(n))/n=O(1a)\n> \n>  * 删除已排序链表中重复的节点\n> \n> 双指针\n\n----------------------------------------\n\n> 打印从1到最大的n位数\n> \n> 常规做法会出现越界问题，考点是字符串的数字加法及终止条件的判定\n\n----------------------------------------\n\n> 数值的整数次方\n> \n> 解法：先举例子\n\n----------------------------------------\n\n> 二进制中1的个数\n> \n> 解法：输入的二进制数与0001进行与运算，若为1则说明最低位存在1，接着左移一位，得到0010，即与输入的二进制数的次低位进行与运算，判断次低位是否为1，以此类推\n\n----------------------------------------\n\n> 反转链表\n> \n> 解法：由于当前节点和下个节点的顺序反转后，链表出现了断裂，即下个节点与下下个节点之间的链接出现断裂，因此需要记录三个节点的信息，\n> \n> prev_node = nullptr\n> while(p->node!=nullptr)\n> {\n> next_node = p->node->next\n> p->node->next = prev_node\n> prev_node = p->node\n> p->node = next_node\n> }\n\n----------------------------------------\n\n> 合并两个排序链表\n> \n> 解法：比较两个链表头节点的大小，将小的一个作为合并后链表的头节点，接着继续合并链表中的剩余节点\n> \n> mergehead\n> if phead1->value < phead2->value:\n> \tmergehead=phead1\n> \tmergehead=\n\n----------------------------------------\n\n> 树的子结构\n\n**\n\n> 二叉树的镜像",
      "normalizedContent": "#\n\n----------------------------------------\n\n 1. 跨层参数共享\n\n> 优点：需保存的参数量降低，减少存储成本。减少训练时间\n> \n> 缺点：推理时间仍然一样\n\n> 共享不同参数层的结果\n\n 2. 词向量因式分解\n    \n    > 引入一全连接层，将词向量维度降维，达到降低transformer中隐层维度的目的\n\n 3. 渐进式知识蒸馏\n    \n    > mobilebert:学生模型开始学习教师模型的第一层，接下来学习教师模型的第二层，而此时学生模型的第一层权重是不参与更新的。依此类推。\n\n 4. 注意力机制的改进（longformer）\n    \n    >  * 滑动窗口注意力\n    >    \n    >    当前token仅与左右k/2个token窗口内的token计算注意力\n    > \n    >  * 扩张窗口注意力\n    >    \n    >    在窗口内不是与所有token计算注意力而是进行采样，与窗口内被采样到的token计算注意力（采样间隔）\n    > \n    >  * 全局+滑动窗口\n    >    \n    >    将qkv映射两种不同的子空间，这两种不同的qk,分别计算全局（与具体任务有关，如具有全局视角的cls，则该token可以关注其他所有token的信息）和滑动窗口注意力\n\n\n\n\n# idea\n\n----------------------------------------\n\n 1. 缺失模态下的情感分析\n\n> 首先，预训练，基于mlm策略（动态掩码（robeta）/静态掩码）\n> \n> 接着，基于预训练好的的重建模型，提取特征，模态融合模块，进行情感预测\n\n 2. 相关的掩码策略（nlp，bart）\n\n * 单词掩码，在输入文本中随机采样一部分单词，并且替换为掩码标记如[mask]\n\n * 单词删除，随机采样一部分单词并且删除。在预测缺失单词的同时还需要确定缺失单词的位置\n\n * 句子排列变换。将多个句子的顺序随机打乱\n\n * 文档旋转变换\n\n * 文本填充\n\n * 实体级别掩码，在普通的单词掩码中，模型只需要根据j.与rowling就可以做出正确的预测，而无法学习到更高层次的语义\n   \n   \n   \n   \n\n 3. 门控策略\n\n * 输入信号经非线性变换映射至[0,1]，得到门控向量，与原始输入进行元素点乘\n\n 4. bert相关\n\n * 词向量编码，块编码，位置编码\n   \n   > 词向量编码由独热向量经由词向量矩阵（可学习参数矩阵）映射得到\n   > \n   > 块编码指的是当前token属于哪一句子，属于句子0则编码为0，属于句子1则编码为1，以此类推。运用块向量矩阵进行映射得到最终的块编码\n   > \n   > 位置编码则是将按单词顺序转换为独热编码，再经由位置向量矩阵转换为位置编码\n\n * mlm实现\n   \n   在得到掩码位置的隐向量后，经由词向量矩阵，映射到词表空间，之后经softmax操作得到词表中的词对应该掩码位置的概率，取max即为预测的单词，模型对第i个掩码位置的预测概率的计算如下： $$ p(i)=softmax(h_{i}w_{}^{t}+b) $$\n   \n   > 整词掩码：掩码的最小粒度为单词级而非wordpiece级，提升了mlm任务的难度，模型在预测掩码词时，需要更多依靠上下文信息，有助于提升模型对输入文本语义信息的挖掘\n   > \n   > 需要注意的是，整词掩码中，子词的掩码方式可以是替换为[mask]，随机词或者保留原词，三选一；各个子词均会受到掩码；掩码的方式受到概率控制\n\n * 下游任务\n   \n   抽取式阅读理解：将问题和篇章输入至模型，问题在前，篇章在后，篇章超过最大允许输入长度时，分段输入。在获取得到每个位置起点、终点位置的概率，分别取top-k个，假设起点终点位置相互独立，则联合概率为二者相乘，得到一个[start,end,prob]的三元组，数量为k*k个，添加限制条件start<end，筛选得到最终的结果。\n\n\n# code\n\n----------------------------------------\n\n 1. 链表\n    \n    > 环的入口节点，如果一链表中包含环，如何确定环的起点\n    > \n    > \n    > \n    > 解法：双指针，通过快慢指针找到处于环中的节点，快指针每次走两步，慢指针每次走一步，由于链表存在环，因此最终肯定会碰上（快的会追上慢的，rectify:快指针多走n步，随后两个指针每次都是走一步），重复的位置就是处于环中的节点。接着如果快慢指针每次走的步长相差为环的长度的整数倍，则最终必会在入口处碰上(rectify:初始时，快指针多走环的长度的步长，接着每次都走一步)，问题转化为寻找环的长度的问题，环的长度可以通过单指针实现，在步骤一中得到的起始节点出发，走一遍，并进行计数，最终就能得到环的长度。接着进行步骤二就能找到入口节点。\n\n----------------------------------------\n\n> 链表中倒数第k个节点\n> \n> 解法：双指针，快指针初始多走k（k-1）步，接着每次都走一步，快指针到终点时，慢指针刚好指向倒数第k个节点。\n\n----------------------------------------\n\n> 调整数组顺序使奇数位于偶数前面\n> \n> 解法：双指针，奇偶（分别向中心移动），奇奇（while到奇数左指针+1），偶偶（右指针-1），偶奇（交换）\n> \n> 2，1，4，5，3\n> \n> 3，1，4，5，2\n> \n> 3，1，5，4，2\n\n----------------------------------------\n\n>  * 删除链表的节点：给定单向链表的头指针和一个节点指针，定义一个函数在o(1)时间内删除该节点\n> \n> 解法：有了要删除的节点指针，可以知道该节点的下一个节点，把下一个节点的值赋值给要删除的节点，将当前节点的next指向下下个节点，除此之外还需要把待删除节点指向的下一个节点删除。倘若在尾部，只能顺序查找删除\n> \n> \n> \n> 时间复杂度的分析\n> \n> ((n-1)*o(1)+o(n))/n=o(1a)\n> \n>  * 删除已排序链表中重复的节点\n> \n> 双指针\n\n----------------------------------------\n\n> 打印从1到最大的n位数\n> \n> 常规做法会出现越界问题，考点是字符串的数字加法及终止条件的判定\n\n----------------------------------------\n\n> 数值的整数次方\n> \n> 解法：先举例子\n\n----------------------------------------\n\n> 二进制中1的个数\n> \n> 解法：输入的二进制数与0001进行与运算，若为1则说明最低位存在1，接着左移一位，得到0010，即与输入的二进制数的次低位进行与运算，判断次低位是否为1，以此类推\n\n----------------------------------------\n\n> 反转链表\n> \n> 解法：由于当前节点和下个节点的顺序反转后，链表出现了断裂，即下个节点与下下个节点之间的链接出现断裂，因此需要记录三个节点的信息，\n> \n> prev_node = nullptr\n> while(p->node!=nullptr)\n> {\n> next_node = p->node->next\n> p->node->next = prev_node\n> prev_node = p->node\n> p->node = next_node\n> }\n\n----------------------------------------\n\n> 合并两个排序链表\n> \n> 解法：比较两个链表头节点的大小，将小的一个作为合并后链表的头节点，接着继续合并链表中的剩余节点\n> \n> mergehead\n> if phead1->value < phead2->value:\n> \tmergehead=phead1\n> \tmergehead=\n\n----------------------------------------\n\n> 树的子结构\n\n**\n\n> 二叉树的镜像",
      "charsets": {
        "cjk": true
      }
    },
    {
      "frontmatter": {},
      "regularPath": "/reading/",
      "relativePath": "reading/README.md",
      "key": "v-28c21a9b",
      "path": "/reading/",
      "headersStr": null,
      "content": "记录一些阅读书籍得到的感悟，以及优美句子等的摘抄",
      "normalizedContent": "记录一些阅读书籍得到的感悟，以及优美句子等的摘抄",
      "charsets": {
        "cjk": true
      }
    },
    {
      "title": "语句摘抄",
      "frontmatter": {},
      "regularPath": "/reading/insights.html",
      "relativePath": "reading/insights.md",
      "key": "v-6fd07596",
      "path": "/reading/insights.html",
      "headersStr": null,
      "content": "# 语句摘抄\n\n 1.  我与众不同，我很独特。无论我外表看起来多么普通，但在心灵深处，我深知自己是独一无二的奇迹。\n\n 2.  不可能一次就中，师兄都没有一次就中，就算不中，也可以换个故事重新投；要求做实验的话，再自己多思考，多交流，网上查找资料进行回复\n\n 3.  要更好实现自身价值，一是始终保持学习和研究事物的热情；二是至少找准一个领域持之以恒、久久为功。\n\n 4.  归纳，总结，思考\n\n 5.  5W1H法则，为什么用法要注意，因为要求回答者追求答案的逻辑性。\n     \n     * 站在对方的立场，换位思考\n     * 提容易回答的问题\n\n 6.  请求别人帮忙的时候，要是能给出一个理由，成功的概率更大。这个理由如果能引起对方的共鸣，那就更加完美\n\n 7.  人们选择那些与自身利益和情绪密切相关的事件运用理智思考，而忽视那些不感兴趣不相干的事凭借本能自动处理。要选择重要的问题并且投入精力，比处理问题还重要。\n\n 8.  交情不够，少说几句\n\n 9.  学会聆听，学会发现对方感兴趣的话题，引导对方说下去\n\n 10. 避免争论，勇于认错，学会赞美\n\n 11. 真心实意感谢他人\n\n 12. 你的心界愈空灵，你也不觉得物界喧嘈，所谓忙中静趣\n\n 13. 人的乐趣一半得之活动（行动起来，诸如运动，清理，培养兴趣等，闲人大半易于发愁），也还有一半得之于感受（欣赏美，感受风景，感受细微，感受生活）\n\n 14. 烂开始，短平快（按周期步骤分解，减小阻力），多迭代（设定每轮迭代的目标）\n     \n     > 提升执行力\n\n 15. 及格是一种心态\n\n 16. 你没喜欢每一个人的义务，你有厌恶任何一个人的理由，但对每一个人，你都要给予基本的尊重，包括你厌恶的人\n\n 17. 如果你的亲人是普通阶层，那对于人生中一些大事来说，他们给的建议往往就是普通阶层的思维，他们的阶层就是他们一生思维决策的结果，如果你的目标是跳出本阶层，那最好只把他们的建议当成参考。\n\n 18. 不要和你的同事比工资，没有意义，比工资总会有人受伤，更多的是负面影响，并且很多时候受伤的会是你。\n\n 19. 不要说一项技能没有用，任何你掌握的技能都有价值，但是你要学会找到发挥它的场景。如果有一天你落水了，你可能会很庆幸，自己以前学会了游泳。\n\n 20. 工作中如果要上升，你要勇于承担麻烦的、有挑战的任务，当你推掉麻烦的时候，你也推掉了机遇。\n\n 21. 很多人喜欢不停的做事，但不会停下来思考，缺乏总结复盘的能力，其实阶段性总结复盘，不仅能够固化前面的经验，也能梳理后面的方向；把事情做对很重要，但是更重要的是做对的事；另外不要贪快，方向正确慢就是快\n\n 22. 不要因为某人的外在，如外貌、习惯、学历等对人贴上标签，去盲目否定别人，对于别人的建议，应该从客观出发，综合分析，从善如流是一项非常难得的品质。",
      "normalizedContent": "# 语句摘抄\n\n 1.  我与众不同，我很独特。无论我外表看起来多么普通，但在心灵深处，我深知自己是独一无二的奇迹。\n\n 2.  不可能一次就中，师兄都没有一次就中，就算不中，也可以换个故事重新投；要求做实验的话，再自己多思考，多交流，网上查找资料进行回复\n\n 3.  要更好实现自身价值，一是始终保持学习和研究事物的热情；二是至少找准一个领域持之以恒、久久为功。\n\n 4.  归纳，总结，思考\n\n 5.  5w1h法则，为什么用法要注意，因为要求回答者追求答案的逻辑性。\n     \n     * 站在对方的立场，换位思考\n     * 提容易回答的问题\n\n 6.  请求别人帮忙的时候，要是能给出一个理由，成功的概率更大。这个理由如果能引起对方的共鸣，那就更加完美\n\n 7.  人们选择那些与自身利益和情绪密切相关的事件运用理智思考，而忽视那些不感兴趣不相干的事凭借本能自动处理。要选择重要的问题并且投入精力，比处理问题还重要。\n\n 8.  交情不够，少说几句\n\n 9.  学会聆听，学会发现对方感兴趣的话题，引导对方说下去\n\n 10. 避免争论，勇于认错，学会赞美\n\n 11. 真心实意感谢他人\n\n 12. 你的心界愈空灵，你也不觉得物界喧嘈，所谓忙中静趣\n\n 13. 人的乐趣一半得之活动（行动起来，诸如运动，清理，培养兴趣等，闲人大半易于发愁），也还有一半得之于感受（欣赏美，感受风景，感受细微，感受生活）\n\n 14. 烂开始，短平快（按周期步骤分解，减小阻力），多迭代（设定每轮迭代的目标）\n     \n     > 提升执行力\n\n 15. 及格是一种心态\n\n 16. 你没喜欢每一个人的义务，你有厌恶任何一个人的理由，但对每一个人，你都要给予基本的尊重，包括你厌恶的人\n\n 17. 如果你的亲人是普通阶层，那对于人生中一些大事来说，他们给的建议往往就是普通阶层的思维，他们的阶层就是他们一生思维决策的结果，如果你的目标是跳出本阶层，那最好只把他们的建议当成参考。\n\n 18. 不要和你的同事比工资，没有意义，比工资总会有人受伤，更多的是负面影响，并且很多时候受伤的会是你。\n\n 19. 不要说一项技能没有用，任何你掌握的技能都有价值，但是你要学会找到发挥它的场景。如果有一天你落水了，你可能会很庆幸，自己以前学会了游泳。\n\n 20. 工作中如果要上升，你要勇于承担麻烦的、有挑战的任务，当你推掉麻烦的时候，你也推掉了机遇。\n\n 21. 很多人喜欢不停的做事，但不会停下来思考，缺乏总结复盘的能力，其实阶段性总结复盘，不仅能够固化前面的经验，也能梳理后面的方向；把事情做对很重要，但是更重要的是做对的事；另外不要贪快，方向正确慢就是快\n\n 22. 不要因为某人的外在，如外貌、习惯、学历等对人贴上标签，去盲目否定别人，对于别人的建议，应该从客观出发，综合分析，从善如流是一项非常难得的品质。",
      "charsets": {
        "cjk": true
      }
    },
    {
      "frontmatter": {},
      "regularPath": "/prepare4work/",
      "relativePath": "prepare4work/README.md",
      "key": "v-7e4fd26a",
      "path": "/prepare4work/",
      "headersStr": null,
      "content": "",
      "normalizedContent": "",
      "charsets": {}
    },
    {
      "title": "技术",
      "frontmatter": {},
      "regularPath": "/tec/",
      "relativePath": "tec/README.md",
      "key": "v-672040e2",
      "path": "/tec/",
      "headersStr": null,
      "content": "",
      "normalizedContent": "",
      "charsets": {}
    },
    {
      "frontmatter": {},
      "regularPath": "/tec/frontend/",
      "relativePath": "tec/frontend/README.md",
      "key": "v-40384bd0",
      "path": "/tec/frontend/",
      "headersStr": null,
      "content": "",
      "normalizedContent": "",
      "charsets": {}
    },
    {
      "frontmatter": {},
      "regularPath": "/tec/frontend/vue.html",
      "relativePath": "tec/frontend/vue.md",
      "key": "v-f704a21a",
      "path": "/tec/frontend/vue.html",
      "headersStr": null,
      "content": "本文总结了在window上基于vuepress搭建个人博客的经历及所踩过的坑。。\n\n最初的设想是先在本地搭建成功之后，再通过github部署，方便个人访问及分享。因此思路主要如下：\n\nstep 1: 通过网络搜索教程，熟悉vuepress相关代码及搭建流程，在个人电脑上进行测试。\n\nstep 2: 在本地部署成功之后，通过个人的github搭建仓库，以sh命令集成指令，自动化提交到分支。\n\n最终没想到在step 1就耗费了非常多的时间和精力，以下总结相关错误及要点：\n\n要点 1：将exports写成export，导致每次新增区块如侧边栏或者导航栏都得不到想要的效果，耗费过于长的时间\n\n总结 1：代码阅读不够仔细\n\nmodule.exports = {\n}\n\n\n要点 2：网上教程是需要通过npm或者yarn安装vuepress，过程中会出现一些bug，如某些库没有安装。\n\n总结 2：仔细阅读bug，定位关键词，通过搜索工具仔细搜寻，得到解决方案。\n\n要点 3：路径设置的问题，包括部署到github需要注意的事项，以及本地搭建是需要对vuepress的路径设置有一定的理解。后者需要阅读 vuepress 官方文档进行准确设置，vuepress 默认会读取该目录下的 README.md 文档，因此每次新建一个文件夹，都需要在该目录下新建一个 README.md：\n\nnav: [\n        {text:'首页',link:\"/\"},//  / 是读取 docs 文件夹下的 README.md 文件，即 / 结尾的路径将会被视为 */README.md\n        {text:'about',link:\"/about/about\"}, // 仅以 / 开头，不写 .md 后缀，默认读取文件夹下对应的 about.md 文件\n        {text:'Github',link:\"https://github.com/ChosenOne23\"}//  跳转链接\n        ],\n\n\n总结 3：通过官方文档准确获取信息的能力。\n\n要点 4：将本地搭建好的博客挂载到 github 上的时候，根据网上的构建 deploy.py 教程，一键自动化部署会有相关问题：\n\n#!/usr/bin/env sh\n# myblog/deploy.sh\n# 确保脚本抛出遇到的错误\nset -e\n\n# 生成静态文件\nnpm run build  # 此处与 myblog/package.json 中的信息需保持一致，注意：默认情况下一般是npm run build\n\n# 进入生成的文件夹\ncd docs/.vuepress/dist\n\ngit init\ngit add -A\ngit commit -m 'deploy'\n\n# 发布到 https://<USERNAME>.github.io/<REPO>\ngit push -f git@github.com:ChosenOne23/myblog.git master:gh-pages # 提交到gh-pages分支\n\ncd -\n\n\n// myblog/package.json\n{\n    ...\n    \"scripts\": {\n        \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\",\n        \"dev\": \"vuepress dev docs --temp .temp\",\n        \"build\": \"set NODE_OPTIONS=--openssl-legacy-provider && vuepress build docs\" // 部署时需加上set NODE_OPTIONS=--openssl-legacy-provider\n      },\n    ...\n}\n\n\n要点 5：图片路径设置问题，在 typora 偏好设置中将图片默认的保存路径设置为../assets，并勾选上以下信息，保证每次插入图片在本地都有存储且方便上传时索引准确。如下：\n\n要点6：为了多设\n\n# 将网页部署到gh-pages分支\nset -e\n# 构建\nnpm run build\n\n# 导航到构建输出目录\ncd docs/.vuepress/dist\n\ngit init\ngit add -A\ngit commit -m 'deploy'\n\n# 推到你仓库的的 gh-page 分支\n# 将 <USERNAME>/<REPO> 替换为你的信息\ngit push -f git@github.com:ChosenOne23/myblog.git master:gh-pages\n\ncd -\n\n\n# 根据远端仓库更新本地分支的命令：\n# 1. 首先克隆(仅需做一次)：\ngit clone -b 分支名 git@github.com:ChosenOne23/myblog.git\n# 2. 更新仓库(远端为 origin)：\ngit pull origin master:master # 将远端的 master 分支更新到本地的master分支\n\n\n# 根据本地分支更新远端分支:\ngit checkout master # b本地切换到master分支\ngit status # 查看当前有变更的代码文件\ngit add ./ # 本地所有修改的内容到暂存区\ngit commit -m “modify_mypc”  # 上传提交信息\ngit pull origin master # 将远程最新的代码先跟你本地的代码合并一下\ngit push origin master # 将代码推至远程就可以了。这里master可以是其他分支名字\n\n\n所以要更新博客，第一步是将远端代码(master分支)与本地(master分支)合并, 第二步是增加或修改博客, 最后一步是将更新后的内容 push 到远端的gh-pages,master",
      "normalizedContent": "本文总结了在window上基于vuepress搭建个人博客的经历及所踩过的坑。。\n\n最初的设想是先在本地搭建成功之后，再通过github部署，方便个人访问及分享。因此思路主要如下：\n\nstep 1: 通过网络搜索教程，熟悉vuepress相关代码及搭建流程，在个人电脑上进行测试。\n\nstep 2: 在本地部署成功之后，通过个人的github搭建仓库，以sh命令集成指令，自动化提交到分支。\n\n最终没想到在step 1就耗费了非常多的时间和精力，以下总结相关错误及要点：\n\n要点 1：将exports写成export，导致每次新增区块如侧边栏或者导航栏都得不到想要的效果，耗费过于长的时间\n\n总结 1：代码阅读不够仔细\n\nmodule.exports = {\n}\n\n\n要点 2：网上教程是需要通过npm或者yarn安装vuepress，过程中会出现一些bug，如某些库没有安装。\n\n总结 2：仔细阅读bug，定位关键词，通过搜索工具仔细搜寻，得到解决方案。\n\n要点 3：路径设置的问题，包括部署到github需要注意的事项，以及本地搭建是需要对vuepress的路径设置有一定的理解。后者需要阅读 vuepress 官方文档进行准确设置，vuepress 默认会读取该目录下的 readme.md 文档，因此每次新建一个文件夹，都需要在该目录下新建一个 readme.md：\n\nnav: [\n        {text:'首页',link:\"/\"},//  / 是读取 docs 文件夹下的 readme.md 文件，即 / 结尾的路径将会被视为 */readme.md\n        {text:'about',link:\"/about/about\"}, // 仅以 / 开头，不写 .md 后缀，默认读取文件夹下对应的 about.md 文件\n        {text:'github',link:\"https://github.com/chosenone23\"}//  跳转链接\n        ],\n\n\n总结 3：通过官方文档准确获取信息的能力。\n\n要点 4：将本地搭建好的博客挂载到 github 上的时候，根据网上的构建 deploy.py 教程，一键自动化部署会有相关问题：\n\n#!/usr/bin/env sh\n# myblog/deploy.sh\n# 确保脚本抛出遇到的错误\nset -e\n\n# 生成静态文件\nnpm run build  # 此处与 myblog/package.json 中的信息需保持一致，注意：默认情况下一般是npm run build\n\n# 进入生成的文件夹\ncd docs/.vuepress/dist\n\ngit init\ngit add -a\ngit commit -m 'deploy'\n\n# 发布到 https://<username>.github.io/<repo>\ngit push -f git@github.com:chosenone23/myblog.git master:gh-pages # 提交到gh-pages分支\n\ncd -\n\n\n// myblog/package.json\n{\n    ...\n    \"scripts\": {\n        \"test\": \"echo \\\"error: no test specified\\\" && exit 1\",\n        \"dev\": \"vuepress dev docs --temp .temp\",\n        \"build\": \"set node_options=--openssl-legacy-provider && vuepress build docs\" // 部署时需加上set node_options=--openssl-legacy-provider\n      },\n    ...\n}\n\n\n要点 5：图片路径设置问题，在 typora 偏好设置中将图片默认的保存路径设置为../assets，并勾选上以下信息，保证每次插入图片在本地都有存储且方便上传时索引准确。如下：\n\n要点6：为了多设\n\n# 将网页部署到gh-pages分支\nset -e\n# 构建\nnpm run build\n\n# 导航到构建输出目录\ncd docs/.vuepress/dist\n\ngit init\ngit add -a\ngit commit -m 'deploy'\n\n# 推到你仓库的的 gh-page 分支\n# 将 <username>/<repo> 替换为你的信息\ngit push -f git@github.com:chosenone23/myblog.git master:gh-pages\n\ncd -\n\n\n# 根据远端仓库更新本地分支的命令：\n# 1. 首先克隆(仅需做一次)：\ngit clone -b 分支名 git@github.com:chosenone23/myblog.git\n# 2. 更新仓库(远端为 origin)：\ngit pull origin master:master # 将远端的 master 分支更新到本地的master分支\n\n\n# 根据本地分支更新远端分支:\ngit checkout master # b本地切换到master分支\ngit status # 查看当前有变更的代码文件\ngit add ./ # 本地所有修改的内容到暂存区\ngit commit -m “modify_mypc”  # 上传提交信息\ngit pull origin master # 将远程最新的代码先跟你本地的代码合并一下\ngit push origin master # 将代码推至远程就可以了。这里master可以是其他分支名字\n\n\n所以要更新博客，第一步是将远端代码(master分支)与本地(master分支)合并, 第二步是增加或修改博客, 最后一步是将更新后的内容 push 到远端的gh-pages,master",
      "charsets": {
        "cjk": true
      }
    },
    {
      "title": "top",
      "frontmatter": {},
      "regularPath": "/tec/linux/linux.html",
      "relativePath": "tec/linux/linux.md",
      "key": "v-77e96299",
      "path": "/tec/linux/linux.html",
      "headers": [
        {
          "level": 3,
          "title": "top",
          "slug": "top",
          "normalizedTitle": "top",
          "charIndex": 2
        },
        {
          "level": 3,
          "title": "tmux",
          "slug": "tmux",
          "normalizedTitle": "tmux",
          "charIndex": 188
        }
      ],
      "headersStr": "top tmux",
      "content": "# top\n\n----------------------------------------\n\ntop 命令用来查看服务器资源使用情况。命令行输入top，显示结果如下：分别展示了各个进程的内存使用情况；运行时间等信息。\n\n要按照内存从大到小排列则输入: shift+m\n\n输入c,全部显示command列（可能出现部分遮挡的情况，可以适当拖动窗口）\n\n\n\n按q退出\n\n\n# tmux\n\n----------------------------------------\n\n * 较全面的网站：快捷键网站\n\n * 认为常用的：\n   \n   * 查看所有会话：tmux ls\n   \n   * 新建窗口：tmux new -s\n   \n   * 分离会话：exit\n   \n   * 重新连接会话：tmux at -t\n   \n   * 切换会话：tmux switch -t\n   \n   * 杀死会话：tmux kill-sesstion -t\n\n * 出现大量...的情况：\n\n解决方案：ctrl+b, shift+d选择分辨率最小的：https://blog.csdn.net/qq_16763983/article/details/130450323",
      "normalizedContent": "# top\n\n----------------------------------------\n\ntop 命令用来查看服务器资源使用情况。命令行输入top，显示结果如下：分别展示了各个进程的内存使用情况；运行时间等信息。\n\n要按照内存从大到小排列则输入: shift+m\n\n输入c,全部显示command列（可能出现部分遮挡的情况，可以适当拖动窗口）\n\n\n\n按q退出\n\n\n# tmux\n\n----------------------------------------\n\n * 较全面的网站：快捷键网站\n\n * 认为常用的：\n   \n   * 查看所有会话：tmux ls\n   \n   * 新建窗口：tmux new -s\n   \n   * 分离会话：exit\n   \n   * 重新连接会话：tmux at -t\n   \n   * 切换会话：tmux switch -t\n   \n   * 杀死会话：tmux kill-sesstion -t\n\n * 出现大量...的情况：\n\n解决方案：ctrl+b, shift+d选择分辨率最小的：https://blog.csdn.net/qq_16763983/article/details/130450323",
      "charsets": {
        "cjk": true
      }
    },
    {
      "frontmatter": {},
      "regularPath": "/tec/linux/",
      "relativePath": "tec/linux/README.md",
      "key": "v-8998e71c",
      "path": "/tec/linux/",
      "headersStr": null,
      "content": "",
      "normalizedContent": "",
      "charsets": {}
    },
    {
      "title": "Typosa快捷键",
      "frontmatter": {},
      "regularPath": "/tec/typora/grammar.html",
      "relativePath": "tec/typora/grammar.md",
      "key": "v-29039e2f",
      "path": "/tec/typora/grammar.html",
      "headers": [
        {
          "level": 2,
          "title": "Typosa快捷键",
          "slug": "typosa快捷键",
          "normalizedTitle": "typosa快捷键",
          "charIndex": 2
        }
      ],
      "headersStr": "Typosa快捷键",
      "content": "# Typosa快捷键\n\n----------------------------------------\n\n> 还是github主题好看😄\n\n * 较全面的快捷键的网站：快捷键网站\n * 我认为的比较常用的：\n   * 选中当前格式的文本：ctrl+e\n   * 选中当前词：ctrl+d\n   * 选中当前行：ctrl+l\n   * 标题：ctrl+1/2/3/4/5\n   * 段落：ctrl+0\n   * 表格：ctrl+t\n   * 代码块：ctrl+shift+k\n   * 公式块：ctrl+shift+m\n   * 引用：ctrl+shift+q\n   * 有序列表：ctrl+shift+[\n   * 无序列表：ctrl+shift+]\n   * 加粗：ctrl+b\n   * 斜体：ctrl+I(大写的i)\n   * 超链接：ctrl+k\n   * 图像：ctrl+shift+I(大写的i)\n   * 大纲视图：ctrl+shift+1\n   * 打字机模式：f9\n   * 主题：alt+a\n   * 分割线：***",
      "normalizedContent": "# typosa快捷键\n\n----------------------------------------\n\n> 还是github主题好看😄\n\n * 较全面的快捷键的网站：快捷键网站\n * 我认为的比较常用的：\n   * 选中当前格式的文本：ctrl+e\n   * 选中当前词：ctrl+d\n   * 选中当前行：ctrl+l\n   * 标题：ctrl+1/2/3/4/5\n   * 段落：ctrl+0\n   * 表格：ctrl+t\n   * 代码块：ctrl+shift+k\n   * 公式块：ctrl+shift+m\n   * 引用：ctrl+shift+q\n   * 有序列表：ctrl+shift+[\n   * 无序列表：ctrl+shift+]\n   * 加粗：ctrl+b\n   * 斜体：ctrl+i(大写的i)\n   * 超链接：ctrl+k\n   * 图像：ctrl+shift+i(大写的i)\n   * 大纲视图：ctrl+shift+1\n   * 打字机模式：f9\n   * 主题：alt+a\n   * 分割线：***",
      "charsets": {
        "cjk": true
      }
    },
    {
      "frontmatter": {},
      "regularPath": "/tec/typora/",
      "relativePath": "tec/typora/README.md",
      "key": "v-7be3be90",
      "path": "/tec/typora/",
      "headersStr": null,
      "content": "",
      "normalizedContent": "",
      "charsets": {}
    }
  ],
  "themeConfig": {
    "nav": [
      {
        "text": "首页",
        "link": "/"
      },
      {
        "text": "about",
        "link": "/about/about"
      },
      {
        "text": "Github",
        "link": "https://github.com/ChosenOne23"
      }
    ],
    "sidebar": [
      {
        "title": "技术",
        "path": "/tec/",
        "collapsable": true,
        "children": [
          {
            "title": "linux",
            "path": "/tec/linux/linux"
          },
          {
            "title": "frontend",
            "path": "/tec/frontend/vue"
          },
          {
            "title": "typora",
            "path": "/tec/typora/grammar"
          }
        ]
      },
      {
        "title": "实验",
        "path": "/exp/",
        "collapsable": true,
        "children": [
          {
            "title": "训练指令",
            "path": "/exp/training_order"
          },
          {
            "title": "想法",
            "path": "/exp/idea"
          }
        ]
      },
      {
        "title": "论文",
        "path": "/paper/",
        "collapsable": true,
        "children": [
          {
            "title": "阅读的相关文章",
            "path": "/paper/paper"
          },
          {
            "title": "文章的撰写",
            "path": "/paper/paper_writing"
          }
        ]
      },
      {
        "title": "prepare4work",
        "path": "/prepare4work/pre4work"
      },
      {
        "title": "阅读",
        "path": "/reading/insights"
      }
    ]
  }
}